\section{Introduction}
\acrfull{rl} is the field of \acrlong{ai} that focuses on training an intelligent agent to make sequential decisions in an environment in order to maximize a specific goal or reward. It is inspired by how humans and animals learn through trial and error, by taking actions and receiving feedback. The agent does not know \textit{a priori} which actions to take, but it must instead discover the ones that yield the highest reward by trying them. In some cases, actions affect not only the immediate reward but also the next state and, through that, all subsequent rewards \cite{rlbook,rlbook2}.

Unfortunately, despite the progresses made in the field, pioneered by the seminal works on Deep \acrlong{rl} \cite{dqn1,dqn2}, \acrshort{rl} is ridden with problems that still make it an unfeasible paradigm for most \acrfull{ml} tasks. \acrshort{rl} is sample inefficient, requiring hundreds of millions of steps for benchmarks such as the Atari Learning Environment (ALE) \cite{atari}, corresponding to tens to hundreds of hours of human play experience \cite{rldoesntwork,rainbow}. Good reward functions that encourage the actual wanted behavior are difficult to design, and prone to fail in unexpected ways \cite{rldoesntwork,concrete_problems_in_ai_safety}; the final results are unstable and hard to reproduce due to random chance generating too much variance, and thus playing a major role during training \cite{drl_that_matters,rldoesntwork}. Randomness can also lead the agents to get stuck in local optima from which it is nigh-on-impossible to escape for the rest of the training phase \cite{rldoesntwork}.

In recent years, the Transformer architecture has reshaped the landscape of \acrshort{ai}, since it is extremely well-suited for applications that deal with sequential data, such as Natural Language Processing \cite{gpt2,gpt3} and Computer Vision \cite{swin_transformer}. By being able to process sequences as a whole rather than token by token, Transformers do not suffer from long dependency issues and do not have to deal with past information, since there are no past states to account for.

In this paper I explore the impact of using a Transformer architecture as the first layer of a \acrfull{dqn}, and I train it on chosen toy environments in the context of online\footnote{In an online setting, instead of relying on previously collected data, the agent explores the environment and it collects experiences by actively interacting with it.}, off-policy\footnote{The agent learns not only from the current interaction, but only from previous interactions that have been gathered with a different policy.} learning. The key idea behind it is that a set of observations can be seen as a sequence on which a Transformer architecture can be trained. Furthermore, the multi-head attention mechanism can help capture what experiences from the set are the most important.

In the end I show that the architecture I propose, called \acrfull{tqn}, is not only able to learn and converge to the optimal policy over time, but it is also able to yield a higher score than the baseline on 1 of the 3 chosen environments. The results are further corroborated following the statistical framework proposed by \texttt{rliable} \cite{rliable}.
