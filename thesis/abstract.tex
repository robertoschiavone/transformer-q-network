\begin{abstract}

In recent years, the Transformer architecture has revolutionized how we process sequential data, but its applications in online \acrshort{rl} settings are still very limited. In this paper a Transformer architecture is applied as the first layer inside a \acrshort{dqn} in order to process in parallel a set of observations. The hypothesis is that the encoder-decoder architecture can extract more information from an observation than a linear layer. Furthermore, the multi-head attention can help the network focus on what experiences are deemed most important. In the end I provide detailed results, backed by a recently suggested statistical framework, to show that the proposed architecture surpasses the baseline on 1 of the 3 classic control environments used for evaluation, but on average it performs worse, providing a good starting point for future research.

\keywords{Reinforcement Learning \and Transformers \and Classic control}
\end{abstract}
